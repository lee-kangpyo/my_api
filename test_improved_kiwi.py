#!/usr/bin/env python3
"""
ê°œì„ ëœ Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
"""

import re
import json
import numpy as np
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
from kiwipiepy import Kiwi

def extract_improved_keywords(text, kiwi):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜"""
    
    # 1. ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ ë¨¼ì € ì¶”ì¶œ
    number_unit_patterns = [
        r'\d+ê°œì›”',  # 3ê°œì›”
        r'\d+km',    # 5km  
        r'\d+ë¶„',    # 30ë¶„
        r'\d+ì‹œê°„',  # 2ì‹œê°„
        r'\d+ì¼',    # 7ì¼
        r'\d+ì£¼',    # 4ì£¼
        r'\d+ë…„',    # 1ë…„
        r'\d+ì ',    # 900ì 
        r'\d+íšŒ',    # 10íšŒ
        r'\d+ë²ˆ',    # 5ë²ˆ
    ]
    
    number_units = []
    for pattern in number_unit_patterns:
        matches = re.findall(pattern, text)
        number_units.extend(matches)
    
    # 2. Kiwië¡œ ëª…ì‚¬ ì¶”ì¶œ
    result = kiwi.analyze(text)
    nouns = [token.form for token in result[0][0] if token.tag.startswith('N') and len(token.form) > 1]
    
    # 3. ë™ì‚¬, í˜•ìš©ì‚¬ë„ ì¶”ì¶œ (ì˜ë¯¸ìˆëŠ” ê²ƒë“¤)
    verbs_adjs = [token.form for token in result[0][0] 
                  if (token.tag.startswith('V') or token.tag.startswith('A')) 
                  and len(token.form) > 1]
    
    # 4. ë¶ˆìš©ì–´ ì œê±°
    stop_words = {'ë‚´ì—', 'ì•ˆì—', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¥¼', 'ì„', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì™€', 'ê³¼'}
    
    # 5. ìµœì¢… í‚¤ì›Œë“œ ì¡°í•©
    all_keywords = []
    all_keywords.extend(number_units)  # ìˆ«ì+ë‹¨ìœ„ ìš°ì„ 
    all_keywords.extend([noun for noun in nouns if noun not in stop_words])
    all_keywords.extend([word for word in verbs_adjs if word not in stop_words][:3])  # ì£¼ìš” ë™ì‚¬/í˜•ìš©ì‚¬ 3ê°œë§Œ
    
    # 6. ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    unique_keywords = []
    for keyword in all_keywords:
        if keyword not in unique_keywords:
            unique_keywords.append(keyword)
    
    return unique_keywords

def test_improved_kiwi():
    """ê°œì„ ëœ Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸš€ ê°œì„ ëœ Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        # Kiwi ë¡œë“œ
        print("ğŸ¥ Kiwi ë¡œë“œ ì¤‘...")
        kiwi = Kiwi()
        print("âœ… Kiwi ë¡œë“œ ì™„ë£Œ!")
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
        test_texts = [
            "3ê°œì›” ë‚´ì— 5km ëŸ¬ë‹ì„ 30ë¶„ ë‚´ì— ì™„ì£¼í•˜ê¸°",
            "í•œ ë‹¬ ì•ˆì— ì˜ì–´ ë©´ì ‘ ì™„ë²½ ëŒ€ë¹„í•˜ê¸°", 
            "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆë¶€í„° ê³ ê¸‰ê¹Œì§€ ë§ˆìŠ¤í„°í•˜ê¸°",
            "í† ìµ 900ì  ë‹¬ì„±ì„ ìœ„í•œ ì²´ê³„ì  í•™ìŠµ ê³„íš",
            "ì£¼ 3íšŒ í—¬ìŠ¤ì¥ì—ì„œ ê·¼ë ¥ ìš´ë™í•˜ê¸°",
            "ë§¤ì¼ 1ì‹œê°„ì”© ë…ì„œ ìŠµê´€ ë§Œë“¤ê¸°"
        ]
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ({len(test_texts)}ê°œ):")
        for i, text in enumerate(test_texts, 1):
            print(f"  {i}. {text}")
        
        print(f"\nğŸ” ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼:")
        
        for i, text in enumerate(test_texts):
            print(f"\n  í…ìŠ¤íŠ¸ {i+1}: {text}")
            
            # ê¸°ì¡´ ë°©ì‹
            result = kiwi.analyze(text)
            old_nouns = [token.form for token in result[0][0] if token.tag.startswith('N')]
            print(f"  ê¸°ì¡´ ëª…ì‚¬: {old_nouns}")
            
            # ê°œì„ ëœ ë°©ì‹
            improved_keywords = extract_improved_keywords(text, kiwi)
            print(f"  ê°œì„ ëœ í‚¤ì›Œë“œ: {improved_keywords}")
            
            # ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ìˆ«ì+ë‹¨ìœ„ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜)
            weights = {}
            for keyword in improved_keywords:
                if re.match(r'\d+\w+', keyword):  # ìˆ«ì+ë‹¨ìœ„
                    weights[keyword] = 2.0
                elif len(keyword) > 2:  # ê¸´ ë‹¨ì–´
                    weights[keyword] = 1.5
                else:
                    weights[keyword] = 1.0
            
            print(f"  ê°€ì¤‘ì¹˜: {weights}")
            
            if i >= 2:  # ì²˜ìŒ 3ê°œë§Œ ìì„¸íˆ
                break
        
        # MariaDB ì €ì¥ í˜•íƒœ ì˜ˆì‹œ
        print(f"\nğŸ’¾ ê°œì„ ëœ MariaDB ì €ì¥ í˜•íƒœ:")
        first_keywords = extract_improved_keywords(test_texts[0], kiwi)
        
        sparse_data = {
            "keywords": ", ".join(first_keywords),
            "weights": {kw: (2.0 if re.match(r'\d+\w+', kw) else 1.5 if len(kw) > 2 else 1.0) 
                       for kw in first_keywords},
            "extraction_method": "kiwi_improved",
            "patterns_used": ["number_unit", "nouns", "verbs_adjs"],
            "total_keywords": len(first_keywords)
        }
        
        print(f"  {json.dumps(sparse_data, indent=2, ensure_ascii=False)}")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("âœ… ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ ì¶”ì¶œ (3ê°œì›”, 5km, 30ë¶„)")
        print("âœ… ì˜ë¯¸ìˆëŠ” ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ ì¶”ì¶œ")
        print("âœ… ë¶ˆìš©ì–´ ì œê±° ë° ê°€ì¤‘ì¹˜ ë¶€ì—¬")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_improved_kiwi()
